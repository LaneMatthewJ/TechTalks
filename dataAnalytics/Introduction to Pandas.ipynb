{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "Pandas is a library for data analysis and manipulation. It sits on top of numpy, and uses all of the functionality that we learned already. Two very powerful tools within the pandas libraries are Series and Dataframes. To learn more about what's possible with pandas, check out their [site]('https://pandas.pydata.org'), and [documentation]('https://pandas.pydata.org/pandas-docs/stable/'). \n",
    "\n",
    "## Installation: \n",
    "\n",
    "If you've installed your environment via downloading anaconda, you likely already have pandas installed. To check, enter the below command into your console: \n",
    "\n",
    "`conda list | grep pandas`\n",
    "\n",
    "Otherwise, if you're using pip, install pandas with the below command via your command line: \n",
    "\n",
    "`pip install pandas`\n",
    "\n",
    "Now let's import the data. For the sake of not having to write pd.<whatever>, we'll be importing series, and dataframe separately from pandas (to save on typing): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Series\n",
    "\n",
    "Series are a very powerful tool within pandas. Series are ultimately a wrapper on top of the numpy, but instead of just using arrays and matrices like in numpy, with series, we get to index our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fibonacciNumbers = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
    "\n",
    "mySeries = Series(fibonacciNumbers)\n",
    "print(mySeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening above is that we have a list that we're passing into series, and we're getting back an indexed list of fibonacci numbers. To access these numbers, we'll use the indices just like how we would use an array for now. Let's grab the 7th fibonacci number: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySeries[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not always the case that you'll want to have your series indexed by numbers though. Let's try being a bit more creative: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudonyms = [\"scary\", \"posh\", \"sporty\", \"ginger\", \"baby\"]\n",
    "names = [\"Mel B\", \"Victoria Beckham\", \"Mel C\", \"Geri Halliwell\", \"Emma Bunton\"]\n",
    "\n",
    "spiceGirls = Series(names, index=pseudonyms)\n",
    "spiceGirls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike arrays, we don't want grab any of our data by numerical index anymore because, while our numerical index exists, we've added new indices for easier access. Ultimately, think of our index now as a dictionary (from the intro to python notebook): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiceGirls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiceGirls['scary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find whether a specific person (or any key for that matter) is in a series, you need to use the `in` keyword: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'posh' in spiceGirls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that our series is now acting entirely like a dictionary, it is actually possible to convert the series itself to a dictionary (granted, by doing so, you would be losing a lot of functionality): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiceGirlsDictionary = spiceGirls.to_dict()\n",
    "spiceGirlsDictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can go from a series to a dictionary, it is also 100% the case that we can take any dictionary we have already created from somewhere else, and make a series out of it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beastieBoysDictionary = {\n",
    "    \"mikeD\": \"Michael Diamond\", \n",
    "    \"MCA\": \"Adam Yauch\", \n",
    "    \"Ad-Rock\": \"Adam Horovitz\"\n",
    "}\n",
    "\n",
    "beastieBoySeries = Series(beastieBoysDictionary)\n",
    "beastieBoySeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to being able to search for boolean values in numpy, we can do the same in series. Let's try it first with some numerical data (let's shift to age): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsyncAges = {\n",
    "    \"justinT\": 38,\n",
    "    \"chrisK\": 47,\n",
    "    \"joeyF\": 42,\n",
    "    \"lanceB\":  40,\n",
    "    \"jcC\": 43\n",
    "}\n",
    "\n",
    "nsyncSeries = Series(nsyncAges)\n",
    "nsyncSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olderThan40 = nsyncSeries > 40\n",
    "olderThan40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsyncSeries[olderThan40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also the case that you can add series together just like numpy arrays. By doing this, you're adding like to like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsyncSeries + nsyncSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at that again, but this time without Justin Timberlake: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsyncAgesWithoutJT = {\n",
    "    \"chrisK\": 47,\n",
    "    \"joeyF\": 42,\n",
    "    \"lanceB\":  40,\n",
    "    \"jcC\": 43\n",
    "}\n",
    "nsyncMinusJT = Series(nsyncAgesWithoutJT)\n",
    "nsyncMinusJT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add them together again to see what happens: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newNsyncSeries = nsyncMinusJT + nsyncSeries\n",
    "newNsyncSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're adding like with like, what's happening is that because there is no `justinT` in the second series, we're trying to add 38 to a number that doesn't exist in the second, so we wind up with `NaN`\n",
    "\n",
    "You'll often want to find whether data don't exist in a given series. To do that, you'll need to use the pandas `pd.isna()` function to get a boolean array: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(newNsyncSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the opposite with `pd.notna()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.notna(newNsyncSeries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're getting a boolean array back, we can filter on that data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notMissingNsync = pd.notna(newNsyncSeries)\n",
    "\n",
    "newNsyncSeries[notMissingNsync]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not always the case that you'll want to filter a series on a given number though. You can filter a series on anything that can return a truth value! Let's go back to beastie boys and filter on whether or not their names contain Adam. For this, we'll want to use the `seriesname.str.contains()` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beastieBoySeries.str.contains('Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedAdam = beastieBoySeries.str.contains('Adam')\n",
    "\n",
    "beastieBoySeries[namedAdam]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series are very powerful datastructures, however, it's unlikely you'll be coming across a singular series of data. Most datasets out there are extremely large with many different features! Let's take a look at DataFrames: \n",
    "\n",
    "## DataFrames\n",
    "\n",
    "You can think of dataframes in multiple ways. I personally like to think of dataframes as a giant spreadsheet that's living in memory, and being powered by a series of series (think of how we had the numpy matrix). \n",
    "\n",
    "Let's start with importing our dataframe from pandas and alias it as df: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame as df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create your own dataframes in almost any number of ways. Let's take a look at creating our own from scratch, then we'll pull a dataframe from the internet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batmanDF = pd.DataFrame({\n",
    "               'born': [pd.Timestamp('1952-10-13'), \n",
    "                        pd.Timestamp('1984-04-17'),\n",
    "                        None, \n",
    "                        pd.Timestamp('1943-02-04')],\n",
    "                'name': ['Alfred', \n",
    "                         'Bruce Wayne', \n",
    "                         '????', \n",
    "                         'Victor Fries'],\n",
    "                'pseudonym': [None, \n",
    "                          'Batman', \n",
    "                          'Joker', \n",
    "                          'Mr. Freeze']})\n",
    "\n",
    "batmanDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we've created a very small dataframe from a dictionary with the keys as columns and their arrays as the row data (where the 0th index refers to Alfred, the 1st to Bruce Wayne, and so on). We can access each individual column by accessing its column name like a dictionary key: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batmanDF['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes pandas' dataframes so powerful is that we can use what we have learned with series and numpy and apply it to much larger datasets. Let's take a look when we get a boolean array from `batmanDF['name']` and apply it to our whole dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isBruceWayne = batmanDF['name'] == 'Bruce Wayne'\n",
    "isBruceWayne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batmanDF[isBruceWayne]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What just happened above is that we got a boolean array that returned true on a single object, and then when applied to our dataframe, we got that specific row! Equally, we can do the exact opposite with a `!=` (and this time, we'll do everything inline): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batmanDF[ batmanDF['name'] != 'Bruce Wayne' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the very basics of DataFrames, let's get a little bit more involved with our dataframes by calling an api and getting some star wars character data. To get this data, we'll have to import `requests`.\n",
    "\n",
    "To install requests, if you have anaconda installed you should already have it (otherwise, type `conda install requests`), and if you're using pip, just enter into the command line: \n",
    "\n",
    "`pip install requests` \n",
    "\n",
    "Then we import it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://swapi.co/api/people/\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our response data, we need to get it into a useable format. Luckily, pandas is really good at reading lots of formats, and can easily take in JSON format). If you're interested in learning more about JSON, check out their [webpage]('https://www.json.org').  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonResponse = response.json()\n",
    "jsonResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonBody = jsonResponse[\"results\"]\n",
    "jsonBody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF = DataFrame(jsonBody)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had wanted to save the reseponse into a file and read it back into the dataframe from the file system, we'd want to use `pd.read_json`. \n",
    "\n",
    "Let's start dipping into our data and see what we got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of data in this dataframe, so let's see what we've got going for us by just looking at the columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many characters we've receieved from our API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(starWarsDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 seems a little low for a series of multiple movies. Let's take a look at our whole dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a little hard to suss through all of the above dataframe, so let's just see what specific characters we've recieved from the api call: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we didn't grab every single star wars character in the entire saga. Let's take a look at our response again just to make sure of what we're getting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good to double check what's in our data before we start working with it! We can see that there are 87 characters, but we only grabbed 10 in our dataframe. There's one key that we should absolutely take note of, and that's the `next` key. In order to keep our api calls quick, the data have been paginated into segments of 10 characters, so we'll need to make 9 calls in total! First, let's make a call to see what the next page is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPageURI = jsonResponse[\"next\"]\n",
    "nextPageURI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPage = requests.get(nextPageURI)\n",
    "nextPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextPageJSON = nextPage.json()\n",
    "nextPageJSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we've got a whole new page of new materials! Let's add it to a dataframe and combine the two! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondPageDataframe = DataFrame(nextPageJSON['results'])\n",
    "secondPageDataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add two dataframes together, we can just concatenate them by placing each of them as an element in an array, and concatenating the array. Let's take a look at both, then concatenate them to see what we get: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondPageDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameList = [ starWarsDF, secondPageDataframe ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superList = pd.concat(dataFrameList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got two dataframes converged into one! This means we can do the same for every single page! Though, first, before we move forward with getting the rest, look closely at the indices of the new dataframe. We have repeating indices of each dataframe. It's not ideal to have multiple values with the same index, so let's reindex these values with the method `reset_index`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superList.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've reset our index, however, notice that we've stored the old index in our dataframe now. It's not always the case that you'll want to overwrite the dataframe's indices, so you might want to keep them, but for our sake, we do want to overwrite, so let's give it a go with the parameter `drop` set to `True`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resetIndexDataFrame = superList.reset_index(drop=True)\n",
    "resetIndexDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything's set back to a `0-(n-1)` list. We could keep doing this over and over again for our API calls, however, it's not really efficient. In order to not do this over and over ad infinitum, we want to automate the tedious things! Let's create a function that can iterate over the entire list and make consecutive calls, ultimately returning to us a list of dataframes (that we can then use to concat and reset our indices): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTheCallsAndReturnTheWholeList():\n",
    "    nextURI = \"https://swapi.co/api/people/\" \n",
    "    dataFrameList = list()\n",
    "\n",
    "\n",
    "    while(nextURI): \n",
    "        response = requests.get(nextURI)\n",
    "        jsonResponse = response.json()\n",
    "\n",
    "        nextURI = jsonResponse['next']\n",
    "        jsonBody = jsonResponse[\"results\"]\n",
    "\n",
    "        dataFrame = DataFrame(jsonBody)\n",
    "        dataFrameList.append(dataFrame)\n",
    "        \n",
    "    \n",
    "    wholeDataFrame = pd.concat(dataFrameList)\n",
    "    resetIndexDF = wholeDataFrame.reset_index(drop=True)\n",
    "    return resetIndexDF\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDataFrame = makeTheCallsAndReturnTheWholeList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's doublecheck that we've actually got all 87 of our characters. We know from the jsonResponse that we're supposed to have 87. Let's double check with `len()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(starWarsDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go any further, let's also double check that we've made the right calls and that we've got a unique list of star wars characters! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueNames = starWarsDataFrame['name'].unique()\n",
    "len(uniqueNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before we move on, let's take a moment to refactor some code. Our function above is great, but it's very likely we'll want to use some of what we used above later on, so let's refactor! Our function origionally looked like: \n",
    "\n",
    "```python\n",
    "def makeTheCallsAndReturnTheWholeList():\n",
    "    nextURI = \"https://swapi.co/api/people/\" \n",
    "    dataFrameList = list()\n",
    "\n",
    "\n",
    "    while(nextURI): \n",
    "        response = requests.get(nextURI)\n",
    "        jsonResponse = response.json()\n",
    "\n",
    "        nextURI = jsonResponse['next']\n",
    "        jsonBody = jsonResponse[\"results\"]\n",
    "\n",
    "        dataFrame = DataFrame(jsonBody)\n",
    "        dataFrameList.append(dataFrame)\n",
    "        \n",
    "    \n",
    "    wholeDataFrame = pd.concat(dataFrameList)\n",
    "    resetIndexDF = wholeDataFrame.reset_index(drop=True)\n",
    "    return resetIndexDF\n",
    "```\n",
    "\n",
    "There are two things our function is doing: \n",
    "    1. Making a call and getting a body of information back\n",
    "    2. Concatenating the list of dataframes together. \n",
    "    \n",
    "Let's start with the making the call and retrieving the json body and the next URI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponseBodyAndNext(URI):\n",
    "    response = requests.get(URI)\n",
    "    jsonResponse = response.json()\n",
    "\n",
    "    nextURI = jsonResponse['next']\n",
    "    jsonBody = jsonResponse[\"results\"]\n",
    "    \n",
    "    return (jsonBody, nextURI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to see if this function works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(filmList, nextURI) = getResponseBodyAndNext('https://swapi.co/api/films')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know our first function works, let's redefine our function so it looks a little cleaner: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTheCallsAndReturnTheWholeList(nextURI):\n",
    "    dataFrameList = list()\n",
    "\n",
    "    while(nextURI): \n",
    "        (jsonBody, nextURI) = getResponseBodyAndNext(nextURI)\n",
    "        dataFrame = DataFrame(jsonBody)\n",
    "        dataFrameList.append(dataFrame)\n",
    "\n",
    "    wholeDataFrame = pd.concat(dataFrameList)\n",
    "    resetIndexDF = wholeDataFrame.reset_index(drop=True)\n",
    "    return resetIndexDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check to see if our function works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsPeopleTest = makeTheCallsAndReturnTheWholeList(\"https://swapi.co/api/people/\")\n",
    "starWarsPeopleTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got our setup done, let's take a look at what's in our dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDataFrame.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not always the case that you'll be wanting all of the data that come from an api (for our purposes, we don't even care about metadata such as \"created and edited\"), so let's drop a couple of columns. The axis parameter is specifying whether or not we want to drop a specific column index, or a specific row index: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF.drop(['created'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be likely to think that since we've called \"drop\" we therefore have dropped the dataframe. You'd be wrong. Pandas functions don't typically follow in-place mutations, and follow a generally functional framework. You can work with this in multiple ways: \n",
    "\n",
    "1. You can create a brand new dataframe (recommended)\n",
    "2. You can use the inplace parameter. \n",
    "3. You can self assign the dataframe after the function call (but this is not recommended)\n",
    "\n",
    "If you are going to mutate your dataframe, I'd recommend using the inplace modifier. \n",
    "\n",
    "Two things of note below: \n",
    "1. Notice that instead of one column, we're now sending in two? The column list is an array and can take as many columns as you can put in.\n",
    "2. We're going to be using the inplace parameter set to true, so that that once we call the funciton we will be dropping our columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF.drop(['created', 'edited'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've removed the metadata for the dataframe, let's take a look at some of our other data. Notice that some of our data are actually urls: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starWarsDF[['films', 'homeworld', 'species', 'starships', 'vehicles']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with this data, we'll need an example of each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmUrl = starWarsDataFrame['films'][0][0]\n",
    "filmUrl\n",
    "getResponseBodyAndNext(filmUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we've run into a potential problem. Not every response from the api will return a next. In many language, that'll just return with a null, however, python will crash if we try to access a key that isn't there. Let's iterate on our method and use a special library called pydash (very similar to lodash for javascript). \n",
    "\n",
    "In order to get pydash, we'll have to install it (it is not part of the conda install). To properly install it (if you have anaconda installed), write: \n",
    "\n",
    "```conda install -c conda-forge pydash```\n",
    "\n",
    "If you have not installed anaconda, you can install pydash by typing: \n",
    "\n",
    "``` pip install pydash ```\n",
    "\n",
    "\n",
    "Once finished installing pydash, you can import it like any other package. For our purposes, however, we'll only be using the `get` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydash import get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pydash get is a method that allows for us to safely retrieve data. If that data does not exist, you can specify a default value to return without worry for your program crashing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponseBodyAndNext(URI):\n",
    "    response = requests.get(URI)\n",
    "    jsonResponse = response.json()\n",
    "\n",
    "    nextURI = get(jsonResponse, 'next', None) # safely get next\n",
    "    jsonBody = jsonResponse[\"results\"]\n",
    "    \n",
    "    return (jsonBody, nextURI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to safely run our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmUrl = starWarsDataFrame['films'][0][0]\n",
    "print(filmUrl)\n",
    "getResponseBodyAndNext(filmUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're seeing that not all of our data that are lists. What we can do is make our function a bit more robust to handle this by using the `type` keyword: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponseBodyAndNext(URI):\n",
    "    response = requests.get(URI)\n",
    "    jsonResponse = response.json()\n",
    "\n",
    "    nextURI = get(jsonResponse, 'next', None) # safely get next\n",
    "    jsonBody = get(get(jsonResponse, 'results')) if isinstance(jsonResponse, list)else jsonResponse\n",
    "    \n",
    "    return (jsonBody, nextURI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getResponseBodyAndNext(filmUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it with the rest of our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homeworldUrl = starWarsDataFrame['homeworld'][0]\n",
    "getResponseBodyAndNext(homeworldUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speciesUrl = starWarsDataFrame['species'][0][0]\n",
    "getResponseBodyAndNext(speciesUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starshipsUrl =  starWarsDataFrame['starships'][0][0]\n",
    "getResponseBodyAndNext(starshipsUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehiclesUrl = starWarsDataFrame['vehicles'][0][0]\n",
    "getResponseBodyAndNext(vehiclesUrl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
